# -*- coding: utf-8 -*-
"""tokenise_speech_dataset.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wg_CPCA-MzsWtsujwy-1Ovhv-tn8Q1nD
"""

import logging
import sys

import pandas as pd
import torch
from snac import SNAC
import torchaudio.transforms as T
from transformers import AutoTokenizer
from datasets import Dataset, Audio


# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler(), logging.FileHandler("finetune.log")]
)
logger = logging.getLogger(__name__)


MANIFEST = "DUMMY1/filtered_manifest.txt"
MAX_AUDIO_FILES = 0
HG_PUSH_NAME = "MbankAI/Orpheus-tokenised-dataset"
ORIGINAL_SAMPLE_RATE = 44100
tokenizer_name = "canopylabs/orpheus-3b-0.1-pretrained"

tokeniser_length = 128256
start_of_text = 128000
end_of_text = 128009

start_of_speech = tokeniser_length + 1
end_of_speech = tokeniser_length + 2

start_of_human = tokeniser_length + 3
end_of_human = tokeniser_length + 4

start_of_ai = tokeniser_length + 5
end_of_ai =  tokeniser_length + 6
pad_token = tokeniser_length + 7

audio_tokens_start = tokeniser_length + 10


def tokenise_audio(waveform):
  waveform = torch.from_numpy(waveform).unsqueeze(0)
  waveform = waveform.to(dtype=torch.float32)
  resample_transform = T.Resample(orig_freq=ORIGINAL_SAMPLE_RATE, new_freq=24000)
  waveform = resample_transform(waveform)

  waveform = waveform.unsqueeze(0).to("cuda")

  #generate the codes from snac
  with torch.inference_mode():
    codes = model.encode(waveform)

  all_codes = []
  for i in range(codes[0].shape[1]):
    all_codes.append(codes[0][0][i].item()+128266)
    all_codes.append(codes[1][0][2*i].item()+128266+4096)
    all_codes.append(codes[2][0][4*i].item()+128266+(2*4096))
    all_codes.append(codes[2][0][(4*i)+1].item()+128266+(3*4096))
    all_codes.append(codes[1][0][(2*i)+1].item()+128266+(4*4096))
    all_codes.append(codes[2][0][(4*i)+2].item()+128266+(5*4096))
    all_codes.append(codes[2][0][(4*i)+3].item()+128266+(6*4096))


  return all_codes

def add_codes(example):
    # Always initialize codes_list to None
    codes_list = None

    try:
        answer_audio = example.get("audio")
        print(answer_audio)
        # If there's a valid audio array, tokenise it
        if answer_audio and "array" in answer_audio:
            audio_array = answer_audio["array"]
            codes_list = tokenise_audio(audio_array)
    except Exception as e:
        print(f"Skipping row due to error: {e}")
        # Keep codes_list as None if we fail
    example["codes_list"] = codes_list

    return example

#@title Create Input Ids
def remove_duplicate_frames(example):
    vals = example["codes_list"]
    if len(vals) % 7 != 0:
        raise ValueError("Input list length must be divisible by 7")

    result = vals[:7]

    removed_frames = 0

    for i in range(7, len(vals), 7):
        current_first = vals[i]
        previous_first = result[-7]

        if current_first != previous_first:
            result.extend(vals[i:i+7])
        else:
            removed_frames += 1

    example["codes_list"] = result

    return example


def create_input_ids(tokenizer, example):
    tok_info = '''*** HERE you can modify the text prompt
    i.e. if you wanted a multispeaker model like canopylabs/orpheus-3b-0.1-ft, you can pass:
    f"{example["source"]}:  {example["text"]}", as is passed.
    '''
    print(tok_info)

    speaker, tone, text  = example["speaker"], example["tone"], example["text"]
    prompt = f"{speaker}:  <{tone}> {text}"

    print("\n\n")
    print("Current prompt being used:")
    print(prompt)
    print("\n")

    text_ids = tokenizer.encode(prompt,  add_special_tokens=True)
    text_ids.append(end_of_text)
    example["text_tokens"] = text_ids
    input_ids = (
        [start_of_human]
        + example["text_tokens"]
        + [end_of_human]
        + [start_of_ai]
        + [start_of_speech]
        + example["codes_list"]
        + [end_of_speech]
        + [end_of_ai]
    )
    example["input_ids"] = input_ids
    example["labels"] = input_ids
    example["attention_mask"] = [1] * len(input_ids)

    return example


if __name__ == '__main__':
    meta_df = pd.read_csv(MANIFEST, sep="|", header=None,
                          names=["path", "speaker", "tone", "unused", "text"])
    meta_df["audio"] = meta_df["path"].astype(str)
    ds = Dataset.from_pandas(meta_df, preserve_index=False)
    ds = ds.cast_column("audio", Audio(sampling_rate=24000))

    print(ds[0]["audio"]["array"].shape)
    print(ds[0]["audio"].keys())
    sys.exit(9876)

    model = SNAC.from_pretrained("hubertsiuzdak/snac_24khz").to("cuda")

    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)

    ds = ds.map(add_codes, remove_columns=["audio"])
    ds = ds.filter(lambda x: x["codes_list"] is not None)
    ds = ds.filter(lambda x: len(x["codes_list"]) > 0)
    ds = ds.map(remove_duplicate_frames, num_proc=1)
    ds = ds.map(lambda x: create_input_ids(tokenizer, x), num_proc=1, remove_columns=["text", "codes_list"])

    columns_to_keep = ["input_ids", "labels", "attention_mask"]
    columns_to_remove = [col for col in ds.column_names if col not in columns_to_keep]
    ds = ds.remove_columns(columns_to_remove)

    #ds.push_to_hub(HG_PUSH_NAME)


